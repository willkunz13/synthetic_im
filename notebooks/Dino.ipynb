{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Dino.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkj32PLZKe6a"
      },
      "source": [
        "# default_exp models.dino"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5DbM5RKKe6s"
      },
      "source": [
        "## models.dino\n",
        "\n",
        "> API details."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aMAXwGDKe6t"
      },
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDq0540AKe6u"
      },
      "source": [
        "#skip\n",
        "!pip install nbdev\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(\"drive/MyDrive/config/secrets.yaml\", 'r') as stream:\n",
        "    secrets = yaml.safe_load(stream)\n",
        "\n",
        "!export AWS_SHARED_CREDENTIALS_FILE=/content/drive/MyDrive/config/awscli.ini\n",
        "path = \"/content/drive/My Drive/config/awscli.ini\"\n",
        "os.environ['AWS_SHARED_CREDENTIALS_FILE'] = path\n",
        "\n",
        "!git clone 'https://{secrets['ACCESS_TOKEN']}@github.com/willkunz13/synthetic_im'\n",
        "!git checkout iter\n",
        "%cd synthetic_im/\n",
        "!pip install -e .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voe9wg8TKe6v"
      },
      "source": [
        "#export\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import synthetic_im.vision_transformer as vision_transformer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeT70QXZKe6v",
        "outputId": "6600d076-d461-4784-913f-bac21d2c3874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#deits8 = torch.hub.load('facebookresearch/dino:main', 'dino_deits8')\n",
        "#torch.save(deits8, 'test_data/models/dino_small.pt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBTJHGz-Ke6w"
      },
      "source": [
        "#export\n",
        "\n",
        "class DinoDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.path = path\n",
        "        self.files = list(Path(self.path).rglob('*.jpg'))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.files[idx]\n",
        "        image = io.imread(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6YMifgqKe6w"
      },
      "source": [
        "data_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "data_dir = 'test_data/cpu_test'\n",
        "image_datasets = DinoDataset(data_dir, data_transforms)\n",
        "                  \n",
        "dataloader = torch.utils.data.DataLoader(image_datasets, batch_size=2,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEA8G3VjoVeO"
      },
      "source": [
        "assert isinstance(dataloader, DataLoader)\n",
        "assert isinstance(dataloader.dataset, DinoDataset)\n",
        "assert device == \"cude:0\" or \"cpu\"\n",
        "\n",
        "assert dataloader.dataset.__getitem__(0).shape == torch.Size([3, 512, 512])\n",
        "assert dataloader.dataset.__getitem__(1).shape == torch.Size([3, 512, 512])\n",
        "try:\n",
        "  assert dataloader.dataset.__getitem__(11).shape == False\n",
        "except:\n",
        "  assert True == True"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YLNOnVkL9Qj"
      },
      "source": [
        "sys.path.insert(0, './test_data/models')\n",
        "sys.path.insert(0, './synthetic_im')\n",
        "\n",
        "model = torch.load('./test_data/models/dino_small.pt')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY05ZyQ_Ke6w"
      },
      "source": [
        "#export\n",
        "def predict_images(model = model, patch_size = 8, threshold = .6, output_dir = Path('data/output'), device=device):\n",
        "    was_training = model.training\n",
        "    for p in model.parameters():\n",
        "      p.requires_grad = False\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(dataloader):\n",
        "            w, h = inputs.shape[2] - inputs.shape[2] % patch_size, inputs.shape[3] - inputs.shape[3] % patch_size\n",
        "            inputs = inputs[:, :, :w, :h]\n",
        "\n",
        "\n",
        "            w_featmap = inputs.shape[-2] // patch_size\n",
        "            h_featmap = inputs.shape[-1] // patch_size\n",
        "\n",
        "            attentions = model.forward_selfattention(inputs.to(device))\n",
        "            bs = attentions.shape[0] # batch size\n",
        "            nh = attentions.shape[1] # number of head\n",
        "\n",
        "            # we keep only the output patch attention\n",
        "            attentions = attentions[:, :, 0, 1:].reshape(bs,nh, -1)\n",
        "\n",
        "            # we keep only a certain percentage of the mass\n",
        "            val, idx = torch.sort(attentions)\n",
        "            val /= torch.sum(val, dim=2, keepdim=True)\n",
        "            cumval = torch.cumsum(val, dim=2)\n",
        "            th_attn = cumval > (1 - threshold)\n",
        "            idx2 = torch.argsort(idx)\n",
        "            for batch in range(bs):\n",
        "                for head in range(nh):\n",
        "                    th_attn[batch,head] = th_attn[batch,head][idx2[batch,head]]\n",
        "            th_attn = th_attn.reshape(bs,nh, w_featmap, h_featmap).float()\n",
        "            # interpolate\n",
        "            th_attn = nn.functional.interpolate(th_attn, scale_factor=patch_size, mode=\"nearest\").cpu().numpy()\n",
        "\n",
        "            attentions = attentions.reshape(bs, nh, w_featmap, h_featmap)\n",
        "            attentions = nn.functional.interpolate(attentions, scale_factor=patch_size, mode=\"nearest\").cpu().numpy()\n",
        "\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            for batch in range(attentions.shape[0]):\n",
        "                out_img = attentions[batch].sum(0)\n",
        "                fname = str(output_dir) + '/attn-' + dataloader.dataset.files[i * bs + batch].name\n",
        "                plt.imsave(\n",
        "                    fname=fname,\n",
        "                    arr=out_img,\n",
        "                    cmap=\"inferno\",\n",
        "                    format=\"jpg\"\n",
        "                )\n",
        "                print(f\"{fname} saved.\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEf592FyqGv"
      },
      "source": [
        "predict_images(output_dir= 'test_data/output', device=\"cpu\")\n",
        "\n",
        "assert os.listdir('test_data/output') != []\n",
        "assert len(os.listdir('test_data/output')) == 2\n",
        "assert plt.imread('test_data/output/attn-1.jpg').shape == (512, 512, 3)\n",
        "\n",
        "dir = 'test_data/output'\n",
        "for f in os.listdir(dir):\n",
        " os.remove(os.path.join(dir, f))"
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}